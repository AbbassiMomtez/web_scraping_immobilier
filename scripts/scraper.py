import csv
import os
import requests
from bs4 import BeautifulSoup

def save_to_csv(data, filename='data/annonces.csv'):
    # Cr√©e le dossier /data s'il n'existe pas d√©j√†
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    # D√©termine si le fichier existe d√©j√†
    file_exists = os.path.isfile(filename)

    # D√©finit les en-t√™tes des colonnes
    fieldnames = ['Titre', 'Prix', 'Localisation', 'Type de bien', 'Date', 'Lien']
    
    with open(filename, mode='a', newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)
        
        # √âcrit les en-t√™tes si le fichier est cr√©√© pour la premi√®re fois
        if not file_exists:
            writer.writeheader()
        
        # √âcrit les donn√©es ligne par ligne
        for annonce in data:
            writer.writerow(annonce)

# URL des annonces immobili√®res
url = "http://www.tunisie-annonce.com/AnnoncesImmobilier.asp"

# Simuler un navigateur avec un User-Agent
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
}

# Envoyer la requ√™te HTTP
response = requests.get(url, headers=headers)

# Liste pour stocker les annonces extraites
annonces_data = []

# V√©rifier si la requ√™te est r√©ussie
if response.status_code == 200:
    soup = BeautifulSoup(response.text, "html.parser")

    # Trouver toutes les annonces (balise <tr class="Tableau1">)
    annonces = soup.find_all("tr", class_="Tableau1")

    for annonce in annonces[:5]:  # Limit√© √† 5 annonces pour tester
        cellules = annonce.find_all("td")  # R√©cup√©rer toutes les colonnes

        if len(cellules) >= 12:  # V√©rifier qu'on a assez de colonnes
            gouvernorat = cellules[1].text.strip()
            type_bien = cellules[3].text.strip()
            titre = cellules[7].find("a").text.strip() if cellules[7].find("a") else "Titre non trouv√©"
            lien = "http://www.tunisie-annonce.com/" + cellules[7].find("a")["href"] if cellules[7].find("a") else "Lien non trouv√©"
            prix = cellules[9].text.strip() if cellules[9] else "Prix non disponible"
            date_publication = cellules[11].text.strip() if cellules[11] else "Date non disponible"
            
            # Ajouter les donn√©es extraites √† la liste
            annonces_data.append({
                'Titre': titre,
                'Prix': prix,
                'Localisation': gouvernorat,
                'Type de bien': type_bien,
                'Date': date_publication,
                'Lien': lien
            })

            # Affichage console pour v√©rifier
            print(f"üè° Titre : {titre}")
            print(f"üìç Localisation : {gouvernorat}")
            print(f"üè† Type de bien : {type_bien}")
            print(f"üí∞ Prix : {prix}")
            print(f"üóìÔ∏è Date : {date_publication}")
            print(f"üîó Lien : {lien}")
            print("-" * 50)

    # Appeler la fonction d'enregistrement en CSV
    save_to_csv(annonces_data)
    print(f"‚úÖ {len(annonces_data)} annonces ont √©t√© sauvegard√©es dans le fichier CSV.")
else:
    print(f"‚ùå √âchec de la connexion. Code HTTP : {response.status_code}")
